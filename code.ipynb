{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a4342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5c07f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16c18c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy(pred, target):\n",
    "    m = target.shape[0]\n",
    "    log_likelihood = -np.log(pred[range(m), target])\n",
    "    return np.sum(log_likelihood) / m\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax_derivative(output, target):\n",
    "    m = target.shape[0]\n",
    "    grad = output.copy()\n",
    "    grad[range(m), target] -= 1\n",
    "    grad /= m\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "472e6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad2d(x, pad):\n",
    "    if pad == 0:\n",
    "        return x\n",
    "    return np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
    "\n",
    "class Conv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        # Xavier initialization\n",
    "        scale = np.sqrt(2.0 / (in_channels * kernel_size * kernel_size))\n",
    "        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * scale\n",
    "        self.biases = np.zeros((out_channels, 1))\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        batch_size, in_channels, in_height, in_width = x.shape\n",
    "        \n",
    "        # Padding\n",
    "        if self.padding > 0:\n",
    "            x_padded = np.pad(x, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)))\n",
    "        else:\n",
    "            x_padded = x\n",
    "        \n",
    "        # Output dimensions\n",
    "        out_height = (in_height + 2*self.padding - self.kernel_size) // self.stride + 1\n",
    "        out_width = (in_width + 2*self.padding - self.kernel_size) // self.stride + 1\n",
    "        \n",
    "        output = np.zeros((batch_size, self.out_channels, out_height, out_width))\n",
    "        \n",
    "        # Vectorized convolution\n",
    "        for b in range(batch_size):\n",
    "            for oc in range(self.out_channels):\n",
    "                for i in range(out_height):\n",
    "                    for j in range(out_width):\n",
    "                        h_start = i * self.stride\n",
    "                        w_start = j * self.stride\n",
    "                        region = x_padded[b, :, h_start:h_start+self.kernel_size, w_start:w_start+self.kernel_size]\n",
    "                        output[b, oc, i, j] = np.sum(region * self.weights[oc]) + self.biases[oc]\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_out, learning_rate=0.01):\n",
    "        batch_size, _, _, _ = d_out.shape\n",
    "        x_padded = np.pad(self.input, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)))\n",
    "        dx_padded = np.zeros_like(x_padded)\n",
    "        d_weights = np.zeros_like(self.weights)\n",
    "        d_biases = np.zeros_like(self.biases)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for oc in range(self.out_channels):\n",
    "                for i in range(d_out.shape[2]):\n",
    "                    for j in range(d_out.shape[3]):\n",
    "                        h_start = i * self.stride\n",
    "                        w_start = j * self.stride\n",
    "                        region = x_padded[b, :, h_start:h_start+self.kernel_size, w_start:w_start+self.kernel_size]\n",
    "                        d_weights[oc] += d_out[b, oc, i, j] * region\n",
    "                        d_biases[oc] += d_out[b, oc, i, j]\n",
    "                        dx_padded[b, :, h_start:h_start+self.kernel_size, w_start:w_start+self.kernel_size] += d_out[b, oc, i, j] * self.weights[oc]\n",
    "\n",
    "        # Remove padding\n",
    "        if self.padding > 0:\n",
    "            dx = dx_padded[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
    "        else:\n",
    "            dx = dx_padded\n",
    "\n",
    "        # Update weights\n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.biases -= learning_rate * d_biases\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28ab60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2D:\n",
    "    def __init__(self, size=2, stride=2):\n",
    "        self.size = size\n",
    "        self.stride = stride\n",
    "        self.input = None\n",
    "        self.max_indices = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        out_height = height // self.size\n",
    "        out_width = width // self.size\n",
    "        output = np.zeros((batch_size, channels, out_height, out_width))\n",
    "        self.max_indices = np.zeros((batch_size, channels, out_height, out_width, 2), dtype=int)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(out_height):\n",
    "                    for j in range(out_width):\n",
    "                        h_start = i * self.size\n",
    "                        w_start = j * self.size\n",
    "                        region = x[b, c, h_start:h_start+self.size, w_start:w_start+self.size]\n",
    "                        max_val = np.max(region)\n",
    "                        output[b, c, i, j] = max_val\n",
    "                        max_idx = np.unravel_index(np.argmax(region), region.shape)\n",
    "                        self.max_indices[b, c, i, j] = [h_start + max_idx[0], w_start + max_idx[1]]\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        batch_size, channels, height, width = self.input.shape\n",
    "        d_input = np.zeros_like(self.input)\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for i in range(d_out.shape[2]):\n",
    "                    for j in range(d_out.shape[3]):\n",
    "                        h, w = self.max_indices[b, c, i, j]\n",
    "                        d_input[b, c, h, w] = d_out[b, c, i, j]\n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97787ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Xavier initialization\n",
    "        scale = np.sqrt(2.0 / input_size)\n",
    "        self.weights = np.random.randn(input_size, output_size) * scale\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        self.output = np.dot(x, self.weights) + self.bias\n",
    "        return relu(self.output)\n",
    "\n",
    "    def backward(self, d_out, learning_rate=0.01):\n",
    "        d_out = d_out * relu_derivative(self.output)\n",
    "        d_input = np.dot(d_out, self.weights.T)\n",
    "        d_weights = np.dot(self.input.T, d_out)\n",
    "        d_bias = np.sum(d_out, axis=0, keepdims=True)\n",
    "\n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.bias -= learning_rate * d_bias\n",
    "        return d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3f33167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self):\n",
    "        self.conv1 = Conv2D(1, 16, kernel_size=3, padding=1)  # Matches PyTorch reference\n",
    "        self.pool1 = MaxPool2D(size=2)\n",
    "        self.conv2 = Conv2D(16, 32, kernel_size=3, padding=1)  # Matches PyTorch reference\n",
    "        self.pool2 = MaxPool2D(size=2)\n",
    "        self.fc1 = FullyConnected(32 * 7 * 7, 128)\n",
    "        self.fc2 = FullyConnected(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1.forward(x)\n",
    "        x = relu(x)\n",
    "        x = self.pool1.forward(x)\n",
    "        \n",
    "        x = self.conv2.forward(x)\n",
    "        x = relu(x)\n",
    "        x = self.pool2.forward(x)\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        return softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "258714b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Predictions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hello\\AppData\\Local\\Temp\\ipykernel_25980\\2028256395.py:46: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  output[b, oc, i, j] = np.sum(region * self.weights[oc]) + self.biases[oc]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_predictions(model, data_loader, num_samples=5):\n",
    "    images, labels = next(iter(data_loader))\n",
    "    images, labels = images.numpy(), labels.numpy()\n",
    "    outputs = model.forward(images)\n",
    "    predictions = np.argmax(outputs, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "        plt.title(f\"Pred: {predictions[i]}\\nTrue: {labels[i]}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "show_predictions(CNN(), test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8911eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "\n",
    "def train(model, train_loader, epochs=5, learning_rate=0.01):\n",
    "    history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images = images.numpy()\n",
    "            labels = labels.numpy()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model.forward(images)\n",
    "            loss = cross_entropy(output, labels)\n",
    "            epoch_loss += loss * len(labels)  # Weight by batch size\n",
    "\n",
    "            # Backward pass\n",
    "            grad = softmax_derivative(output, labels)\n",
    "            grad = model.fc2.backward(grad, learning_rate)\n",
    "            grad = model.fc1.backward(grad, learning_rate)\n",
    "            \n",
    "            # Reshape and backprop through conv/pool layers\n",
    "            grad = grad.reshape(-1, 32, 7, 7)\n",
    "            grad = model.pool2.backward(grad)\n",
    "            grad = model.conv2.backward(grad, learning_rate)\n",
    "            grad = model.pool1.backward(grad)\n",
    "            model.conv1.backward(grad, learning_rate)\n",
    "\n",
    "            # Metrics\n",
    "            predictions = np.argmax(output, axis=1)\n",
    "            correct += np.sum(predictions == labels)\n",
    "            total += len(labels)\n",
    "            \n",
    "            # Progress update\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1} | Batch {batch_idx}/{len(train_loader)} | \"\n",
    "                      f\"Loss: {epoch_loss/total:.4f} | Acc: {correct/total*100:.2f}%\", end='\\r')\n",
    "\n",
    "        # Epoch statistics\n",
    "        epoch_loss /= len(train_loader.dataset)\n",
    "        epoch_acc = correct / len(train_loader.dataset)\n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['accuracy'].append(epoch_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc*100:.2f}% | \"\n",
    "              f\"Time: {time.time()-start_time:.2f}s\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c53a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, labels in data_loader:\n",
    "        images = images.numpy()\n",
    "        labels = labels.numpy()\n",
    "        \n",
    "        output = model.forward(images)\n",
    "        predictions = np.argmax(output, axis=1)\n",
    "        \n",
    "        y_true.extend(labels)\n",
    "        y_pred.extend(predictions)\n",
    "        total_loss += cross_entropy(output, labels) * len(labels)\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Loss: {avg_loss:.4f} | Accuracy: {accuracy*100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, digits=4))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def plot_training(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in history:\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "    if 'val_accuracy' in history:\n",
    "        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e64b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    transform = transforms.ToTensor()\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Initialize and train model\n",
    "    model = CNN()\n",
    "    history = train(model, train_loader, epochs=5)\n",
    "    \n",
    "    # Evaluate and plot\n",
    "    test_acc = evaluate(model, test_loader)\n",
    "    plot_training(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
